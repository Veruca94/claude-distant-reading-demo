# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This repository is a distant reading demonstration project using Shakespeare plays as a corpus. "Distant reading" is a computational approach to literary analysis that examines large collections of texts through quantitative methods, as opposed to traditional close reading of individual works.

## Corpus Contents

The repository contains five Shakespeare plays from Project Gutenberg:

- `pg1513.txt` - Romeo and Juliet
- `pg1514.txt` - A Midsummer Night's Dream
- `pg1519.txt` - Much Ado about Nothing
- `pg1524.txt` - Hamlet
- `pg1533.txt` - Macbeth

All texts are in plain text format with Project Gutenberg headers and formatting intact.

## Architecture

This project implements a complete distant reading analysis pipeline with two main components:

### Python Analysis Pipeline (`analysis/`)
The analysis pipeline consists of modular Python scripts that process the Shakespeare texts:

- **`preprocess.py`**: Strips Project Gutenberg headers/footers and tokenizes text with NLTK
- **`bow_analyzer.py`**: Computes word frequencies and TF-IDF scores for distinctive words
- **`sentiment_analyzer.py`**: Performs VADER sentiment analysis at play and act levels
- **`style_analyzer.py`**: Calculates stylometric metrics (lexical diversity, readability scores, vocabulary richness)
- **`wordcloud_generator.py`**: Creates visual word cloud images using matplotlib
- **`analyze_all.py`**: Main orchestrator that runs all analyses and outputs JSON data

### Web Interface (`web/`)
An interactive single-page application for exploring the analysis results:

- **`index.html`**: Main structure with single-play view and comparison mode
- **`styles.css`**: Responsive styling with gradient themes and chart visualizations
- **`app.js`**: Core application logic for loading data, rendering views, and managing interactions
- **`wordcloud.js`**: Canvas-based dynamic word cloud generator with customizable parameters

### Data Flow
1. Source texts (`pg*.txt`) → Python analysis → JSON output (`output/data/`)
2. Web interface loads JSON → Interactive visualizations
3. Static word clouds saved to `output/wordclouds/`
4. Dynamic word clouds generated in-browser via canvas

## Common Commands

### Running the Analysis Pipeline
```bash
# Install dependencies
pip install -r requirements.txt

# Run complete analysis on all five plays
python3 analysis/analyze_all.py

# Test individual modules
python3 analysis/preprocess.py pg1524.txt
```

### Viewing the Web Interface
```bash
# Option 1: Use provided server script
python3 serve.py
# Then open http://localhost:8000

# Option 2: Use Python's built-in HTTP server
cd web
python3 -m http.server 8000

# Option 3: Open directly (may have CORS issues)
open web/index.html
```

### Development Workflow
1. Modify analysis modules in `analysis/`
2. Run `python3 analysis/analyze_all.py` to regenerate JSON
3. Refresh browser to see updated results
4. No build step required for web interface (vanilla JS)

## Output Structure

### JSON Data Files (`output/data/*.json`)
Each play has a JSON file containing:
- `metadata`: Play title, genre, source file
- `statistics`: Word counts, unique words, sentences
- `bag_of_words`: Top words (with frequencies), distinctive words (TF-IDF), vocabulary stats
- `sentiment`: Overall scores (pos/neg/neu/compound), summary, act-level breakdown
- `style`: Lexical diversity, readability scores, vocabulary richness, sentence statistics
- `wordcloud`: Path to static word cloud image

### Word Cloud Images (`output/wordclouds/*.png`)
Static PNG images (1200x600) generated by Python, used as baseline in web interface.

## Key Implementation Details

### Text Preprocessing
- Project Gutenberg headers are stripped using regex pattern matching for `*** START/END ***` markers
- NLTK's `punkt` tokenizer for sentences and words
- English stopwords from NLTK corpus
- All processing preserves original source files

### Sentiment Analysis
- Uses VADER (Valence Aware Dictionary and sEntiment Reasoner)
- Particularly effective for literary/archaic language
- Compound score ranges from -1 (most negative) to +1 (most positive)
- Act-level sentiment extracted when ACT markers are detected

### Stylometric Features
- **Lexical Diversity**: Type-token ratio (unique words / total words)
- **Hapax Legomena**: Words appearing exactly once (indicates vocabulary richness)
- **Readability**: Multiple metrics (Flesch Reading Ease, Flesch-Kincaid Grade, Gunning Fog, SMOG, etc.)
- Uses `textstat` library for standardized readability calculations

### TF-IDF for Distinctive Words
- Computed across all five plays using scikit-learn
- Identifies words that are frequent in one play but rare in others
- Helps characterize unique vocabulary of each play

### Web Interface State Management
- Vanilla JavaScript with simple global state object
- No framework dependencies (React, Vue, etc.)
- Data loaded asynchronously via `fetch` API
- Dynamic rendering with direct DOM manipulation

## Development Patterns

When modifying the analysis pipeline:
- Keep modules independent and single-purpose
- Return dictionaries from analysis functions for JSON serialization
- Use type hints for better code clarity
- Test modules individually before running full pipeline

When modifying the web interface:
- Follow existing event-driven pattern
- Update state object before triggering view updates
- Keep styling in CSS, avoid inline styles
- Add new visualizations as separate rendering functions
